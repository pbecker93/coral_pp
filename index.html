<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="CoRAL">
  <meta name="keywords" content="Reinforcement Learning, Multi-Sensor Reinforcement Learning, Contrastive Learning, State Space Representations, Image-Based RL, Multimodal Representations">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CoRAL</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--  <link rel="icon" href="./static/images/favicon.svg">-->
<!--  <link rel="icon" href="./static/images/Untitled.ico">-->
    <link rel="icon" href="./static/images/alr-logo_large.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Combining Reconstruction and Contrastive Methods for Multimodal Representations in RL</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href=https://pbecker93.github.io>Philipp Becker</a><sup>1,2</sup>,</span>
            <span class="author-block">
             Sebastian Mossburger</a><sup>1</sup>,</span>
              <span class="author-block">
              Fabian Otto</a><sup>3,4</sup>,</span>
            <span class="author-block">
              <a href=https://alr.iar.kit.edu/21_65.php target=_blank>Gerhard Neumann</a><sup>1,2</sup>
            </span>
          </div>
        </br>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Autonomous Learning Robots (ALR), Karlsruhe Institute of Technology (KIT)</span></br>
            <span class="author-block"><sup>2</sup>FZI Research Center for Information Technology (FZI)</span></br>
            <span class="author-block"><sup>3</sup>Bosch Center for Artificial Intelligence</span></br>
            <span class="author-block"><sup>4</sup>University of TÃ¼bingen</span>
          </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
<div class="column has-text-centered">
  <div class="publication-links">
    <!-- PDF Link. -->
    <!--span class="link-block">
      <a href="https://openreview.net/pdf?id=9ZkUFSwlUH"
         class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
            <i class="fas fa-file-pdf"></i>
        </span>
        <span>Paper</span>
      </a>
    </span-->
    <span class="link-block">
      <a href="https://arxiv.org/abs/2302.05342"
         class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
            <i class="ai ai-arxiv"></i>
        </span>
        <span>arXiv</span>
      </a>
    </span>
    <!-- Code Link. -->
    <span class="link-block">
      <a href="https://github.com/pbecker93/CoRAL"
         class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
            <i class="fab fa-github"></i>
        </span>
        <span>Code</span>
        </a>
    </span>
  </div>
</div>
<section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <img src="./static/images/01_fig_fig1.png">
      <div class="content has-text-justified">

      <b>Figure 1</b>: <i>Contrastive Reconstructive Aggregated representation Learning (CoRAL)</i> learns multimodal state space representations of all available sensors using a combination of reconstruction-based and contrastive objectives.       
      Building on the insight that we can exchange likelihood-based reconstruction with contrastive approaches using mutual information, allows us to choose an appropriate loss function for each modality.  
      Motivated by both a variational and predictive coding viewpoint, <i>CoRAL</i> helps model-free and model-based agents to excel in challenging tasks that require information fusion from sensors with different properties such as images and proprioception.    
      </div>
    </div>
  </div>
</div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
            Learning self-supervised representations using reconstruction or contrastive losses improves performance and sample complexity of image-based and multimodal reinforcement learning (RL). Here, different self-supervised loss functions have distinct advantages and limitations depending on the information density of the underlying sensor modality. Reconstruction provides strong learning signals but is susceptible to distractions and spurious information. While contrastive approaches can ignore those, they may fail to capture all relevant details and can lead to representation collapse. For multimodal RL, this suggests that different modalities should be treated differently based on the amount of distractions in the signal. We propose <i>Contrastive Reconstructive Aggregated representation Learning (CoRAL)</i>, a unified framework enabling us to choose the most appropriate self-supervised loss for each sensor modality and allowing the representation to better focus on relevant aspects. We evaluate <i>CoRAL's</i> benefits on a wide range of tasks with images containing distractions or occlusions, a new locomotion suite, and a challenging manipulation suite with visually realistic distractions. Our results show that learning a multimodal representation by combining contrastive and reconstruction-based losses can significantly improve performance and solve tasks that are out of reach for more naive representation learning approaches and other recent baselines.
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


</body>
</html>